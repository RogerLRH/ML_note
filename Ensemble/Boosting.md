# Boosting

## 基本思想：

1. 先赋予每个训练样本相同的概率

2. 然后进行T次迭代，每次迭代后，对分类错误的样本加大权重(重采样)，使得在下一次的迭代中更加关注这些样本。

3. 把训练出来的一系列弱学习器组合起来，每个弱学习器都有一个相应的权重

## Adaboost

1. 训练第m个弱分类器 $y_m$，最小化权重误差函数

$$\epsilon_m = \sum_{n=1}^N w_n^m I(y_m(x_n) \neq t_n)$$

2. 计算弱分类器权重

$$\alpha_m = ln(\frac {1 - \epsilon_m} {\epsilon_m})$$

3. 更新样本权重

$$w_n^{m + 1} = \frac {w_n^m} {Z_m} exp(- \alpha_m t_n y_m(x_n))$$

4. 最后的分类器

$$y(x) = sign(\sum_{m=1}^{M} \alpha_m y_m(x))$$

## 特点

>每次迭代与上次迭代相关

>每个样本的权重不同，错误的样本权重变大

>每个弱分类器的权重和误差相关

## 优缺点

- 优点

>泛化错误率降低

>易实现

>可以处理不均衡的样本集

>充分考虑每个分类器的权重

- 缺点

>对于离群点很敏感，易过拟合

>无法并行训练

>数据不平衡导致分类精度下降

>迭代次数也就是弱分类器数目不太好设定，可以使用交叉验证来进行确定
