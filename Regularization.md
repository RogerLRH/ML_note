# 正则化

## 参数范数惩罚

#### $L_1$

>相当于在梯度上增加和参数正负号相同的常数，故可能产生稀疏解

#### $L_2$

>相当于缩放参数，强特征保留，弱特征收缩

## 约束

>参数范数惩罚相当于对权重的约束，但无法得知具体的约束值

>若知道怎样的约束值合适，显式的约束更好

>惩罚可能导致局部极小，而显式约束只在权重过大时产生作用

>显式约束的优化过程更稳定

## 数据集增强

>平移、旋转、缩放、加上噪音

## TODO: 噪音鲁棒性

## 半监督学习

>无监督学习表征，监督预测

>权衡监督和非监督的目标函数

## 多任务学习

>共享模型的一部分

>需要任务有相关性

## 提前终止

>模型会过拟合时，训练误差随时间推移逐渐下降，但验证集误差会先降后升

>若在一定循环次数内，验证误差无改善，则停止训练

>需要保存最佳的参数

>有正则化的效果，等价于 $L_2$

## 参数共享

>参见CNN

## 稀疏表示

>权重直接惩罚模型参数，稀疏表示惩罚激活单元，间接对参数施加复杂惩罚

>也可以通过硬性约束获得稀疏表示

## bagging和其它集成方法

>分别训练不同的模型，表决产生结果，也称模型平均

>一般集成方法可以使用不同的算法和不同的目标函数训练成完全不同的模型，bagging 允许多次重复使用完全一样的模型

>每次训练使用的训练集通过重复采样而来，所以每个数据集不同，且高概率缺少部分数据，少量重复数据

## Dropout

>计算方便、功能强大

>通过对单元输出乘于零有效地删除单元

>可以看作集成大量深层神经网络的bagging方法。不同的是：
>>Dropout 的模型共享参数，且每个模型继承部分参数；
>>
>>Dropout 的大部分模型没有显式训练，因为父神经网络太大了，参数共享使得其它网络也有较好的参数设定

>通常使用 0.5 的概率随机删除单元

>权重比例推断规则：一般在训练结束后将权重乘概率，另一种是训练时将单元输出除以概率，保证测试和训练时的每个单元的期望输入是大致相同的

>权重比例推断规则对非线性的模型仅是近似，但实践中效果很好，比蒙特卡罗更优

>对模型和训练过程基本无限制

>数据集过大时，计算代价可能超过正则化带来的好处；数据集过小时，也不会很有效

>对正则化效果来说，随机性不是必要的，也不是充分的。快速 dropout 通过减少梯度计算的随机性获得更快的收敛速度。Dropout Boosting 缺乏正则化效果

>由于隐藏单元的共享，每个隐藏单元必须表现良好，并且适应大多数情况或者说模型

>dropout 强大的大部分原因是添加到隐藏单元的掩码噪音。传统的噪音注入技术，难以进行局部的清除，如在人脸上去除鼻子的信息；而模型若学到通过鼻子检测脸的隐藏单元，丢失该单元使得模型必须学习另一个特征，要么是鼻子的冗余信息，要么是像嘴这样的特征。

## 对抗训练

>对原数据加上较小的噪音，得到截然不同的结果，新的数据称为对抗样本

>对抗样本对不同模型通常是通用的

>对抗训练能显著减少错误率

## TODO: 切面距离、正切传播和流形正切分类器
