# GBDT(Gradient Boosting Decision Tree)/MART(Multiple Additive Regression Tree)

## 构成

#### DT

>回归树，因为分类树没有梯度

#### GB

>每次迭代的回归树，学习的是残差，而不是目标值，预测通过所有回归树叠加

>有时候，直接使用残差很难优化，故使用梯度信息近似为要学习的残差

>使用平方损失函数时，梯度就是残差，这就是通常说的残差版本

#### 步骤

> 计算损失函数的负梯度在当前模型的值，将它作为残差的估计

> 估计回归树叶节点区域，以拟合残差的近似值

> 利用线性搜索估计叶节点区域的值，使损失函数极小化

> 更新回归树

## 优点

>可以发现多种有区分性的特征以及特征组合

>泛化能力较强
