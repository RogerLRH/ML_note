# 决策树

## 构成

>树状，父节点代表特征，边代表特征值，叶子节点表示目标值

## 步骤

>三个步骤：特征选择、决策树生成、剪枝

## 特征选择

#### 信息增益（ID3）

$$Entropy(S) = - \sum_{S_k \in Type(S)} \frac {|S_k|} {|S|} log_2 (\frac {|S_k|} {|S|})$$

$$Gain(S,A) = Entropy(S) - \sum_{v\in Value(A)} \frac {|S_v|} {|S|} Entropy(S_v)$$

>定义：由于选择该属性导致的信息熵降低

>缺点：倾向于取值较多的特征

#### 信息增益比（C4.5）

$$SplitInformation(S,A) = - \sum_{v \in Value(A)} \frac {|S_v|} {|S|} log_2 \frac {|S_v|} {|S|}$$

$$GainRatio(S,A) = \frac {Gain(S,A)} {SplitInformation(S,A)}$$

>缺点：倾向于取值较少的特征

#### 基尼指数（CART）

$$Gini(p) = \sum_{k=1}^K p_k (1 - p_k) = 1 - \sum_{k=1}^K p_k^2$$

$$Gini(D,A) = \frac {D_1} {D} Gini(D_1) + \frac {D_2} {D} Gini(D_2)$$

>CART是二叉树

## 剪枝

#### 过拟合

>噪音数据

>缺少代表性数据

#### 预剪枝

>构造时，若效果提升过小，或节点样本个数过少，或树的深度过大，则剪枝

#### 误差降低剪枝

$$E = e / N$$

$$E' = \sum_{i} e_i / \sum_i N_i$$

>叶子结点上级节点替换为叶子结点，比较验证集中的优劣，若更优，则正式替换

>从下而上的方式遍历

>上述计算在额外的验证集上进行

#### 悲观错误剪枝

$$E = (e + \epsilon)/ N$$

$$E'= \sum_{i} (e_i + \epsilon) / \sum_i N_i$$

>不需要验证集

>叶子结点上级节点替换为叶子结点，错误率应该上升；通过加入惩罚，使替换成为可能。通常在每个叶子结点加上 $\epsilon = 0.5$ 的错误数

>从上而下的方式遍历

#### 最小误差剪枝

$$E = \frac {e + k - 1} {N + k}$$

$$E'= \sum_{i} \frac {N_i} {N} \frac {e_i + k - 1} {N_i + k}$$

>从下而上的方式遍历

#### 代价复杂剪枝

>以节点个数为复杂度，加入惩罚

## 连续值处理

>以信息增益作特征处理时，容易倾向于连续值特征；但以信息增益比，在类别平衡时的分界点使得增益比的抑制最大；故决定特征时该使用增益比，而选择分界点时使用增益

## 缺失值处理

#### 缺失情况

>1. 选择分类特征时，训练样本缺失特征
>
>2. 确定分类特征后，训练样本缺失特征
>
>3. 预测样本缺失特征

#### 应对

>1. 忽略样本
>
>2. 赋予均值或中值
>
>3. 计算增益或增益比时根据缺失占比对增益或增益比进行惩罚
>
>4. 根据其它特征补全
>
>5. 根据有属性的样本的子集占比，把特征补全
>
>6. 把缺失样本分配到所有子集
>
>7. 单独把缺失样本分到一个分支

#### 对应

>1: 1,2,3,4
>
>2: 1,2,4,5,6,7
>
>3: 2,4,7

## 优点

>决策树易于理解和解释

>可以同时处理标签型和数值型数据

>预测比较快

## 缺点

>对缺失数据处理比较困难

>容易出现过拟合问题

>忽略数据集中属性的相互关联
